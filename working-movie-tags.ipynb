{"cells":[{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2023-01-18T13:56:46.358867Z","iopub.status.busy":"2023-01-18T13:56:46.358495Z","iopub.status.idle":"2023-01-18T13:56:51.795974Z","shell.execute_reply":"2023-01-18T13:56:51.794864Z","shell.execute_reply.started":"2023-01-18T13:56:46.358836Z"},"id":"5SLSr69bvPy7","trusted":true},"outputs":[],"source":["from transformers import AutoTokenizer\n","import pandas as pd\n","import tensorflow as tf\n","\n","import os"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2023-01-18T13:56:51.799432Z","iopub.status.busy":"2023-01-18T13:56:51.798627Z","iopub.status.idle":"2023-01-18T13:56:58.876736Z","shell.execute_reply":"2023-01-18T13:56:58.875734Z","shell.execute_reply.started":"2023-01-18T13:56:51.799389Z"},"id":"KkGrEz7tvPzC","trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a2c0c15e182b4d62b613fcf5aa6f6892","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"70c9e57dbda8485581db383ddd5535e7","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a0e5f579b34f406682038bbabb9937d9","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/226k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"08e018c63bbb4021bbbcb2d97e6facf7","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/455k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"}],"source":["tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2023-01-18T13:57:01.208307Z","iopub.status.busy":"2023-01-18T13:57:01.207948Z","iopub.status.idle":"2023-01-18T13:57:03.316956Z","shell.execute_reply":"2023-01-18T13:57:03.316050Z","shell.execute_reply.started":"2023-01-18T13:57:01.208276Z"},"id":"3mXFEOgnvPzD","outputId":"be9be1aa-6a96-449b-b011-ccecf1249faa","trusted":true},"outputs":[{"data":{"text/plain":["Index(['imdb_id', 'title', 'plot_synopsis', 'tags', 'split',\n","       'synopsis_source'],\n","      dtype='object')"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["df = pd.read_csv(\"/kaggle/input/mpst-movie-plot-synopses-with-tags/mpst_full_data.csv\")\n","df.columns"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2023-01-18T13:57:03.319185Z","iopub.status.busy":"2023-01-18T13:57:03.318825Z","iopub.status.idle":"2023-01-18T13:57:03.333801Z","shell.execute_reply":"2023-01-18T13:57:03.332689Z","shell.execute_reply.started":"2023-01-18T13:57:03.319149Z"},"trusted":true},"outputs":[{"data":{"text/plain":["imdb_id            0\n","title              0\n","plot_synopsis      0\n","tags               0\n","split              0\n","synopsis_source    0\n","dtype: int64"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["df.isna().sum()"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2023-01-18T13:57:06.784719Z","iopub.status.busy":"2023-01-18T13:57:06.783659Z","iopub.status.idle":"2023-01-18T13:57:06.801616Z","shell.execute_reply":"2023-01-18T13:57:06.800606Z","shell.execute_reply.started":"2023-01-18T13:57:06.784671Z"},"id":"eHfzGIVOvPzE","trusted":true},"outputs":[],"source":["train = df.loc[df[\"split\"]==\"train\"]\n","test = df.loc[df[\"split\"]==\"test\"]\n","val = df.loc[df[\"split\"]==\"val\"]\n"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"execution":{"iopub.execute_input":"2023-01-18T13:57:06.977436Z","iopub.status.busy":"2023-01-18T13:57:06.977058Z","iopub.status.idle":"2023-01-18T13:57:06.987062Z","shell.execute_reply":"2023-01-18T13:57:06.985932Z","shell.execute_reply.started":"2023-01-18T13:57:06.977406Z"},"id":"LxitLWkQvPzF","outputId":"0fa9f46b-fce7-41fb-8dc9-2e9da34fb12c","trusted":true},"outputs":[{"data":{"text/plain":["'cult, horror, gothic, murder, atmospheric'"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["train.tags[0]"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":337},"execution":{"iopub.execute_input":"2023-01-18T13:57:07.532206Z","iopub.status.busy":"2023-01-18T13:57:07.531072Z","iopub.status.idle":"2023-01-18T13:57:07.551253Z","shell.execute_reply":"2023-01-18T13:57:07.550389Z","shell.execute_reply.started":"2023-01-18T13:57:07.532156Z"},"id":"nxjCH17hvPzG","outputId":"f085c771-71a4-4cb9-84f0-d61c1a757421","trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>imdb_id</th>\n","      <th>title</th>\n","      <th>plot_synopsis</th>\n","      <th>tags</th>\n","      <th>split</th>\n","      <th>synopsis_source</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>tt0057603</td>\n","      <td>I tre volti della paura</td>\n","      <td>Note: this synopsis is for the orginal Italian...</td>\n","      <td>cult, horror, gothic, murder, atmospheric</td>\n","      <td>train</td>\n","      <td>imdb</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>tt1733125</td>\n","      <td>Dungeons &amp; Dragons: The Book of Vile Darkness</td>\n","      <td>Two thousand years ago, Nhagruul the Foul, a s...</td>\n","      <td>violence</td>\n","      <td>train</td>\n","      <td>imdb</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>tt0113862</td>\n","      <td>Mr. Holland's Opus</td>\n","      <td>Glenn Holland, not a morning person by anyone'...</td>\n","      <td>inspiring, romantic, stupid, feel-good</td>\n","      <td>train</td>\n","      <td>imdb</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>tt0249380</td>\n","      <td>Baise-moi</td>\n","      <td>Baise-moi tells the story of Nadine and Manu w...</td>\n","      <td>gothic, cruelty, violence, cult, revenge, sadist</td>\n","      <td>train</td>\n","      <td>wikipedia</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>tt0408790</td>\n","      <td>Flightplan</td>\n","      <td>Kyle Pratt (Jodie Foster) is a propulsion engi...</td>\n","      <td>mystery, suspenseful, action, murder, flashback</td>\n","      <td>train</td>\n","      <td>imdb</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["     imdb_id                                          title  \\\n","0  tt0057603                        I tre volti della paura   \n","1  tt1733125  Dungeons & Dragons: The Book of Vile Darkness   \n","3  tt0113862                             Mr. Holland's Opus   \n","6  tt0249380                                      Baise-moi   \n","7  tt0408790                                     Flightplan   \n","\n","                                       plot_synopsis  \\\n","0  Note: this synopsis is for the orginal Italian...   \n","1  Two thousand years ago, Nhagruul the Foul, a s...   \n","3  Glenn Holland, not a morning person by anyone'...   \n","6  Baise-moi tells the story of Nadine and Manu w...   \n","7  Kyle Pratt (Jodie Foster) is a propulsion engi...   \n","\n","                                               tags  split synopsis_source  \n","0         cult, horror, gothic, murder, atmospheric  train            imdb  \n","1                                          violence  train            imdb  \n","3            inspiring, romantic, stupid, feel-good  train            imdb  \n","6  gothic, cruelty, violence, cult, revenge, sadist  train       wikipedia  \n","7   mystery, suspenseful, action, murder, flashback  train            imdb  "]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["train.head()"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":140},"execution":{"iopub.execute_input":"2023-01-18T13:57:09.250289Z","iopub.status.busy":"2023-01-18T13:57:09.249920Z","iopub.status.idle":"2023-01-18T13:57:09.257670Z","shell.execute_reply":"2023-01-18T13:57:09.256594Z","shell.execute_reply.started":"2023-01-18T13:57:09.250259Z"},"id":"xSugnthgEvHj","outputId":"190e02d7-605d-40c0-983c-aa6ef8e8df6c","trusted":true},"outputs":[{"data":{"text/plain":["\"Matuschek's, a gift store in Budapest, is the workplace of Alfred Kralik (James Stewart) and the newly hi Ed\\nKlara Novak (Margaret Sullavan). At work they constantly irritate each other, but this daily aggravation is tempered by the fact that each has a secret pen pal with which they trade long soul-searching letters. Romantic correspondence is sent back and forth, and while Alfred and Klara trade barbs at work, they dream of someday meeting their sensitive, caring and unknown pen pal.Christmas is fast approaching, and the store is busy. Alfred had been with the store for some time, and has always been treated well by Mr. Matuschek (Frank Morgan), but lately his attitude has changed. Alfred is at a loss, and Matuschek avoids any explanation, finally telling Alfred that it would be best if he left. Stunned, Alfred accepts his last paycheck and says goodbye to everyone, including Klara. For once they are civil to each other.A long awaited meeting of the secret pen pals was planned for that night, and Alfred having just lost his job has no desire to go. Finding he can't fight his curiosity, he wanders to the restaurant where they'd agreed to meet and peeks in the window with his fellow employee. Of course, Klara is there waiting for him, with the chosen book and wearing a red carnation they'd agreed to use as a signal. Realizing that he'd been wrong about her all along, and that his irritation with her was actually masking his attraction, he finally enters and goes over to her table, but does not reveal his true reason for being there although he is aware she will be hurt that her pen pal doesn't show up. Alfred, hurt by her rudeness, finally leaves, knowing that she will wait all night for someone who is no longer coming.Meanwhile, back at the store, Mr. Matuschek has a late-night meeting with a private detective. He knows that his wife has been having an affair with one of his employees, and was convinced it was his trusted friend, Alfred. The detective however tells Matuschek it is in fact another employee and, heart-broken over his wife's infidelity he retires to his office. The delivery boy, returning late, enters and prevents Matuschek shooting himself with a pistol. Collapsing in grief and shame, Matuschekis rushed to the hospital.The next day Alfred visits Mr. Matuschek in his sick bed, where he asks for Alfred's forgiveness and puts him back to work, now as manager of the store. The delivery boy is rewarded with a raise to a store clerk. Klara arrives at work late, obviously heartbroken after the failure of her correspondent to materialize last night. When she finds Alfred in the manager's office she doesn't believe him and when she discovers it is true she faints in the middle of his office. Later, as she is resting at home, Alfred pays her a visit, and while he is there her aunt brings her another letter from her secret pen pal that explains his not being at the meeting because he saw her there with Alfred. Relieved about the misunderstanding she swears to Alfred she'll be back at work in the morning. Alfred is obviously working on a plan to reveal himself to Klara.Christmas Eve is here, and everyone works through the day. Mr. Matuschek has nearly recovered from his sickness and stops by to see how things are going, and when the final tally is made, the store has had its best sales day since 1928. Delighted, he hands out bonuses to all, and takes the new stock boy out for Christmas dinner. Alfred and Klara are getting ready to leave, and she has another date with her mystery pen pal, but Alfred delays her with a few questions. She's never yet seen him and doesn't even know his name but is convince end she will be engaged when she comes back to work. He tells her that the mysterious pen pal stopped by to see him earlier, and he is in fact fat, bald, older and unemployed and quite willing to live off Klara's income.\\nAlfred reveals himself when he puts a red carnation in his lapel and suddenly eveything becomes clear to her.\""]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["test[\"plot_synopsis\"][2]"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2023-01-18T13:57:10.894647Z","iopub.status.busy":"2023-01-18T13:57:10.893758Z","iopub.status.idle":"2023-01-18T13:57:10.904545Z","shell.execute_reply":"2023-01-18T13:57:10.903593Z","shell.execute_reply.started":"2023-01-18T13:57:10.894599Z"},"id":"uWwP1-3P3c6V","outputId":"6d332a9d-3488-4f33-be0c-54d9bb7b9672","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["14828\n","5604\n"]}],"source":["vals = [i for i in df[\"tags\"]]\n","print(len(vals))\n","vals = set(vals)\n","print(len(vals))"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2023-01-18T13:57:11.232242Z","iopub.status.busy":"2023-01-18T13:57:11.231503Z","iopub.status.idle":"2023-01-18T13:57:11.252759Z","shell.execute_reply":"2023-01-18T13:57:11.251863Z","shell.execute_reply.started":"2023-01-18T13:57:11.232204Z"},"id":"sTZr9fhY3c3R","outputId":"a9faad6d-3a11-4008-f701-58b5bd356da0","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["0\n"]}],"source":["import itertools\n","newl = [item.split(\", \") for item in vals]\n","finl = list(itertools.chain.from_iterable(newl))\n","c=0\n","for item in finl:\n","    if not isinstance(item,str):\n","        c+=1\n","print(c)"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2023-01-18T13:57:11.968099Z","iopub.status.busy":"2023-01-18T13:57:11.967744Z","iopub.status.idle":"2023-01-18T13:57:11.976759Z","shell.execute_reply":"2023-01-18T13:57:11.975729Z","shell.execute_reply.started":"2023-01-18T13:57:11.968072Z"},"id":"MX5YwCYK3c08","outputId":"713cad16-10b4-42e4-ef86-7c6695c91305","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["29391\n","71\n"]}],"source":["print(len(finl))\n","finl = set(finl)\n","print(len(finl))"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2023-01-18T13:57:12.351103Z","iopub.status.busy":"2023-01-18T13:57:12.350141Z","iopub.status.idle":"2023-01-18T13:57:12.357229Z","shell.execute_reply":"2023-01-18T13:57:12.356202Z","shell.execute_reply.started":"2023-01-18T13:57:12.351056Z"},"id":"C7FSsim-4TuV","outputId":"d6f509a1-cc3c-41a8-c79b-25dd9cf42e15","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["{'historical fiction': 0, 'allegory': 1, 'suspenseful': 2, 'alternate history': 3, 'inspiring': 4, 'humor': 5, 'neo noir': 6, 'mystery': 7, 'boring': 8, 'sadist': 9, 'feel-good': 10, 'bleak': 11, 'haunting': 12, 'horror': 13, 'good versus evil': 14, 'cute': 15, 'avant garde': 16, 'dark': 17, 'clever': 18, 'sci-fi': 19, 'atmospheric': 20, 'melodrama': 21, 'murder': 22, 'christian film': 23, 'plot twist': 24, 'brainwashing': 25, 'philosophical': 26, 'paranormal': 27, 'anti war': 28, 'absurd': 29, 'psychedelic': 30, 'cult': 31, 'tragedy': 32, 'dramatic': 33, 'thought-provoking': 34, 'non fiction': 35, 'queer': 36, 'adult comedy': 37, 'comedy': 38, 'entertaining': 39, 'prank': 40, 'whimsical': 41, 'intrigue': 42, 'revenge': 43, 'historical': 44, 'claustrophobic': 45, 'autobiographical': 46, 'insanity': 47, 'realism': 48, 'action': 49, 'pornographic': 50, 'flashback': 51, 'home movie': 52, 'depressing': 53, 'cruelty': 54, 'satire': 55, 'stupid': 56, 'suicidal': 57, 'psychological': 58, 'romantic': 59, 'alternate reality': 60, 'violence': 61, 'storytelling': 62, 'grindhouse film': 63, 'western': 64, 'magical realism': 65, 'blaxploitation': 66, 'fantasy': 67, 'gothic': 68, 'sentimental': 69, 'comic': 70}\n"]}],"source":["dict_map ={}\n","for index,tag in enumerate(finl):\n","    dict_map[tag] = index\n","print(dict_map)"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2023-01-18T13:57:13.156809Z","iopub.status.busy":"2023-01-18T13:57:13.156209Z","iopub.status.idle":"2023-01-18T13:57:13.179578Z","shell.execute_reply":"2023-01-18T13:57:13.178629Z","shell.execute_reply.started":"2023-01-18T13:57:13.156772Z"},"id":"8pHtvkJ5BUZC","outputId":"89f0e05f-8b2c-4b09-9c90-ae5aa75d5cf1","trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Token indices sequence length is longer than the specified maximum sequence length for this model (1663 > 512). Running this sequence through the model will result in indexing errors\n"]},{"data":{"text/plain":["{'input_ids': [101, 3602, 1024, 2023, 19962, 22599, 2003, 2005, 1996, 8917, 13290, 3059, 2713, 2007, 1996, 9214, 1999, 2023, 3056, 2344, 1012, 11235, 6382, 7245, 13999, 2093, 5469, 7122, 1997, 1996, 6097, 7875, 2890, 1998, 1996, 11189, 2124, 2004, 1996, 1005, 2093, 5344, 1997, 3571, 1005, 1012, 1996, 7026, 7352, 2100, 1006, 15954, 21442, 19562, 1007, 2003, 2019, 8702, 1010, 2152, 1011, 21125, 24262, 2655, 1011, 2611, 2040, 5651, 2000, 2014, 22445, 1010, 8102, 4545, 2044, 2019, 3944, 2041, 2043, 2016, 3202, 4152, 2022, 13462, 2011, 1037, 2186, 1997, 4326, 3042, 4455, 1012, 1996, 20587, 2574, 4453, 2370, 2004, 3581, 1010, 2014, 4654, 1011, 14255, 8737, 2040, 2038, 3728, 6376, 2013, 3827, 1012, 26851, 2003, 10215, 2005, 2009, 2001, 2014, 10896, 2008, 5565, 1996, 2158, 1999, 7173, 1012, 2559, 2005, 14017, 10732, 1010, 26851, 11640, 2014, 11690, 7089, 2984, 1006, 1048, 6038, 2850, 24493, 5644, 2072, 1007, 1012, 1996, 2048, 2308, 2031, 2042, 24211, 2005, 2070, 2051, 1010, 2021, 26851, 2003, 3056, 2008, 2016, 2003, 1996, 2069, 2028, 2040, 2064, 2393, 2014, 1012, 2984, 10217, 2000, 2272, 2058, 2008, 2305, 1012, 3823, 2101, 1010, 3581, 4455, 2153, 1010, 10015, 2008, 2053, 3043, 2040, 2016, 4455, 2005, 3860, 1010, 2002, 2097, 2031, 2010, 7195, 1012, 4242, 2000, 26851, 1010, 2984, 2003, 1996, 20587, 17727, 18617, 5844, 3581, 1012, 5914, 8480, 2012, 26851, 1005, 1055, 4545, 2574, 2044, 1010, 1998, 2515, 2014, 2190, 2000, 5475, 26851, 1005, 1055, 10627, 1012, 2016, 3957, 1996, 6634, 1011, 4930, 2450, 1037, 25283, 26147, 28863, 1998, 8509, 2014, 2000, 2793, 1012, 2101, 2008, 2305, 2004, 26851, 25126, 1010, 2984, 4152, 2039, 2041, 1997, 2793, 1010, 1998, 25636, 1037, 3602, 1997, 12633, 1024, 2016, 2001, 1996, 2028, 2437, 1996, 4326, 3042, 4455, 2043, 2016, 4342, 1997, 21310, 4019, 2013, 3827, 1012, 4209, 2008, 26851, 2052, 2655, 2006, 2014, 2005, 2393, 1010, 2016, 7607, 2008, 2016, 2371, 2009, 2001, 2014, 2126, 1997, 2746, 2067, 2046, 2014, 2166, 2044, 2037, 19010, 1012, 2096, 2016, 2003, 5697, 3015, 1010, 2016, 11896, 2000, 5060, 2019, 22841, 1999, 1996, 4545, 1012, 2023, 2051, 2009, 2003, 3581, 1010, 2005, 2613, 1012, 2002, 19815, 2015, 2039, 2369, 2984, 1998, 2358, 21476, 2015, 2014, 2000, 2331, 2007, 2028, 1997, 26851, 2015, 27201, 26412, 1012, 1996, 2614, 1997, 1996, 5998, 8300, 2078, 26851, 1998, 2016, 23813, 1999, 25966, 1012, 1996, 25303, 14255, 8737, 10919, 2008, 2002, 2074, 2730, 1996, 3308, 2450, 1010, 1998, 3254, 3084, 2010, 2126, 2000, 26851, 1005, 1055, 2793, 1012, 2174, 1010, 3041, 2008, 2305, 1010, 26851, 2018, 2872, 1037, 14998, 5442, 2104, 2014, 10005, 2012, 2984, 1005, 1055, 10293, 1012, 26851, 15126, 2015, 1996, 5442, 1998, 17079, 2015, 3581, 2007, 2009, 2004, 2002, 1005, 1055, 2927, 2000, 2358, 21476, 2014, 1012, 26851, 9010, 1996, 5442, 1998, 7807, 2091, 1999, 29004, 1010, 5129, 2011, 1996, 2048, 18113, 1997, 2014, 2280, 10205, 1012, 1996, 8814, 13639, 23451, 2378, 3708, 2301, 3607, 1010, 8748, 1040, 1005, 24471, 7959, 2003, 1037, 2402, 18487, 2006, 1037, 2146, 4440, 1012, 2076, 1996, 2607, 1997, 2010, 4990, 1010, 2002, 4858, 1037, 28923, 11547, 2007, 1037, 5442, 16687, 2046, 2049, 2540, 1012, 2002, 10632, 2015, 1996, 6085, 1998, 3138, 2009, 2004, 1037, 2061, 27346, 4313, 1012, 2101, 2008, 2305, 1010, 8748, 6762, 2012, 1037, 2235, 3541, 9151, 2000, 3198, 2005, 7713, 1012, 2002, 14444, 2195, 24210, 5689, 2039, 2006, 2028, 1997, 1996, 3681, 1010, 1998, 1037, 10030, 2686, 2008, 6433, 2000, 4906, 1996, 2028, 2002, 2038, 3603, 1012, 8748, 2003, 4527, 2011, 1996, 4211, 1997, 17697, 1006, 1043, 17298, 3597, 21058, 8609, 2080, 1007, 1010, 2040, 7607, 2008, 1996, 5442, 7460, 2000, 2010, 2269, 1010, 2040, 2038, 2025, 2042, 2464, 2005, 2274, 2420, 1012, 17697, 4107, 1037, 2282, 2000, 1996, 2402, 4175, 1010, 1998, 3525, 13999, 2032, 2000, 1996, 2717, 1997, 1996, 2155, 1024, 2010, 2564, 1006, 15544, 2912, 13764, 3981, 1007, 1010, 2037, 2402, 2365, 7332, 1010, 17697, 1005, 1055, 3920, 2567, 15541, 1006, 28080, 19838, 4048, 1007, 1010, 1998, 2905, 17371, 2368, 2912, 1006, 10514, 6508, 5143, 1007, 1012, 2009, 3525, 9099, 20781, 2015, 2008, 2027, 2024, 17858, 26481, 1996, 5508, 1997, 2037, 2269, 1010, 2175, 11140, 2050, 1010, 2004, 2092, 2004, 1996, 3114, 2005, 2010, 6438, 1024, 2002, 2038, 2908, 2000, 2079, 2645, 2007, 1996, 19104, 1998, 14436, 2098, 8814, 13639, 23451, 4862, 11693, 1012, 8748, 2003, 5457, 2011, 1996, 2744, 1010, 1998, 17371, 2368, 2912, 7607, 2008, 1037, 8814, 13639, 23451, 2003, 1037, 3788, 28353, 22208, 2040, 14172, 2006, 1996, 2668, 1997, 1996, 2542, 1010, 9544, 8231, 2485, 2814, 1998, 2155, 2372, 1012, 17697, 1998, 15541, 2024, 3056, 2008, 1996, 11547, 8748, 2018, 3603, 2003, 2008, 1997, 4862, 11693, 1010, 2021, 2036, 5382, 2008, 2045, 2003, 1037, 2844, 6061, 2008, 2037, 2269, 2038, 2042, 10372, 2011, 1996, 2668, 8364, 2205, 1012, 2027, 11582, 1996, 4175, 2000, 2681, 1010, 2021, 2002, 7288, 2000, 2994, 1998, 26751, 1996, 2214, 16042, 2709, 1012, 2012, 1996, 6909, 1997, 7090, 1010, 2175, 11140, 2050, 1006, 11235, 6382, 7245, 1007, 5651, 2000, 1996, 9151, 1012, 2010, 14768, 21745, 1998, 4895, 3489, 27718, 3311, 8945, 3207, 1996, 4788, 1010, 1998, 1996, 2048, 3428, 2024, 7950, 1024, 2027, 5382, 2008, 2009, 2003, 2037, 4611, 2000, 3102, 2175, 11140, 2050, 2077, 2002, 14172, 2006, 1996, 2155, 1010, 2021, 2037, 2293, 2005, 2032, 3084, 2009, 3697, 2000, 3362, 1037, 3247, 1012, 2101, 2008, 2305, 1010, 2119, 7332, 1998, 15541, 2024, 4457, 2011, 2175, 11140, 2050, 2040, 18916, 2068, 1997, 2668, 1010, 1998, 2059, 24776, 1996, 9151, 1012, 17697, 7533, 1998, 2022, 13038, 15541, 2000, 4652, 2032, 2013, 7065, 14966, 2004, 1037, 8814, 13639, 23451, 1012, 2021, 2002, 2003, 8729, 2013, 2725, 2061, 2000, 7332, 2043, 2010, 2564, 17016, 2000, 10797, 5920, 1012, 2128, 26896, 5794, 14626, 1010, 2002, 10217, 2000, 11010, 1996, 2775, 2302, 2635, 1996, 4072, 29361, 1012, 2008, 2168, 2305, 1010, 1996, 2775, 9466, 2013, 2010, 6542, 1998, 27591, 2000, 2022, 4778, 2046, 1996, 9151, 1012, 1996, 2388, 3216, 2000, 2014, 2365, 1005, 1055, 4681, 1010, 21690, 17697, 2043, 2002, 4740, 2000, 2644, 2014, 1010, 2069, 2000, 2022, 11188, 2012, 1996, 2392, 2341, 2011, 2175, 11140, 2050, 1012, 1996, 2214, 2158, 9017, 1998, 1999, 25969, 2015, 2010, 2684, 1011, 1999, 1011, 2375, 1010, 2040, 2059, 2515, 1996, 2168, 2005, 2014, 3129, 1012, 8748, 1998, 17371, 2368, 2912, 10574, 2013, 1996, 9151, 1998, 2175, 2006, 1996, 2448, 1998, 5342, 2041, 1999, 1996, 8435, 1997, 2019, 4704, 5040, 2004, 6440, 7807, 1012, 8748, 2003, 21931, 2008, 1037, 2146, 1998, 3407, 2166, 3658, 2007, 2068, 1012, 2021, 17371, 2368, 2912, 2003, 11542, 2000, 2128, 4115, 15549, 4095, 2014, 2155, 7208, 1012, 2016, 7164, 2008, 2016, 2003, 3214, 2000, 2994, 2007, 1996, 2155, 1012, 17371, 2368, 2912, 1005, 1055, 10069, 2055, 2014, 2155, 2024, 4484, 2043, 2008, 3944, 1010, 2175, 11140, 2050, 1998, 2014, 9504, 2265, 2039, 2012, 1996, 4704, 9460, 1012, 2004, 8748, 25126, 1010, 17371, 2368, 2912, 2003, 26673, 2046, 2037, 8295, 2608, 2073, 2027, 6805, 2000, 2331, 1012, 20256, 2011, 2014, 11652, 1010, 8748, 18545, 2000, 2014, 4681, 1010, 2021, 1996, 2155, 2038, 2525, 2579, 2014, 2188, 1010, 6932, 1996, 7089, 2000, 3582, 7621, 1012, 1996, 2402, 18487, 4858, 2014, 1010, 4688, 19917, 2006, 2014, 2793, 1012, 17371, 2368, 2912, 8300, 3619, 1010, 1998, 1037, 5664, 2689, 2003, 5710, 2006, 2014, 2227, 1012, 2053, 2936, 11922, 1010, 8748, 9979, 2015, 2014, 1010, 1998, 2016, 15424, 1998, 1999, 25969, 2015, 2032, 2205, 1012, 1996, 4530, 1997, 2300, 2378, 6652, 2414, 1010, 2563, 1010, 6821, 6330, 8812, 1006, 17551, 5578, 5602, 1007, 2003, 2170, 2000, 1037, 2312, 2160, 2000, 7374, 1996, 11547, 1997, 2019, 9750, 5396, 2005, 2014, 8940, 1012, 2004, 2016, 5102, 1996, 2303, 1010, 2016, 14444, 2019, 9603, 6323, 3614, 2006, 2049, 4344, 1012, 16312, 2011, 22040, 1010, 6821, 8812, 15539, 2009, 1012, 2004, 2016, 2515, 1010, 1037, 3221, 10247, 2058, 1010, 1998, 9010, 1997, 2300, 4088, 2000, 17624, 2006, 1996, 2723, 1012, 2016, 2003, 2036, 4632, 17440, 2011, 1037, 4875, 1010, 2053, 4797, 6296, 2011, 1996, 19255, 1997, 1996, 2303, 1012, 4895, 21678, 3709, 2021, 7537, 2011, 2014, 7654, 1010, 2016, 12321, 1996, 3105, 1998, 5651, 2188, 2000, 2014, 2235, 2264, 2203, 4257, 1012, 2044, 4192, 2188, 1010, 6821, 8812, 2003, 4632, 17440, 2011, 4326, 2824, 1012, 1996, 20386, 4875, 5651, 1998, 4247, 2000, 20739, 2121, 2014, 1012, 2059, 1996, 4597, 1999, 2014, 4545, 2175, 2041, 1010, 1998, 1996, 4165, 1997, 1996, 14309, 2300, 4247, 2007, 24890, 2075, 3180, 3012, 1012, 2016, 5927, 1996, 2214, 2450, 2015, 11547, 4688, 2006, 2014, 2793, 1010, 1998, 2746, 2875, 2014, 1012, 1996, 10215, 2450, 27591, 2005, 17213, 1010, 2021, 2016, 4821, 2358, 21476, 2015, 2841, 1010, 12126, 2008, 1996, 5396, 1005, 1055, 2398, 2024, 13940, 2014, 3759, 1012, 1996, 2279, 2851, 1010, 1996, 9530, 19562, 3351, 1006, 14207, 2317, 19960, 2378, 1007, 9418, 6821, 8812, 1005, 1055, 2303, 1998, 4455, 1996, 2610, 1012, 1996, 14064, 2006, 1996, 3496, 1006, 24801, 2139, 6583, 20683, 1007, 2855, 14730, 2008, 2049, 1037, 3722, 2553, 1998, 2008, 6821, 8812, 1000, 2351, 1997, 25966, 1000, 1012, 1996, 4130, 8662, 8480, 2006, 1996, 3496, 2000, 11628, 1996, 2303, 2077, 2009, 1005, 1055, 2579, 2185, 1998, 2002, 3964, 2008, 1996, 2069, 3696, 1997, 4808, 2003, 1037, 2235, 24851, 2006, 2014, 2187, 4344, 1010, 3262, 3497, 3303, 2043, 2619, 26927, 2098, 1037, 3614, 2013, 2014, 4344, 1012, 2004, 1996, 3460, 3084, 2023, 8089, 1010, 1996, 9530, 19562, 3351, 3544, 24305, 1010, 2004, 2016, 2038, 4593, 2165, 1996, 3614, 2013, 1996, 2757, 6821, 8812, 1010, 1998, 2003, 2582, 11116, 2011, 1996, 2614, 1997, 1037, 4875, 25430, 29046, 2055, 1999, 1996, 2250, 1012, 1012, 1012, 1012, 11235, 6382, 7245, 3084, 1037, 2345, 3311, 2004, 2175, 11140, 2050, 5559, 2006, 2010, 3586, 2004, 2002, 14730, 1996, 2093, 7122, 1997, 3571, 1998, 4136, 1996, 7193, 2000, 2022, 6176, 2096, 3788, 2188, 2012, 2305, 2005, 11277, 1998, 6144, 2031, 2053, 3571, 1012, 1996, 3746, 8005, 2067, 2000, 2941, 7487, 2032, 3564, 2006, 1037, 17678, 8275, 3586, 2007, 1037, 4950, 3626, 1998, 2536, 3626, 3549, 3048, 5628, 2105, 2000, 26633, 1996, 3496, 1997, 5559, 2083, 1996, 3224, 2013, 1996, 8814, 13639, 23451, 6903, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["tokenizer(df[\"plot_synopsis\"][0])"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2023-01-18T13:57:19.436404Z","iopub.status.busy":"2023-01-18T13:57:19.436047Z","iopub.status.idle":"2023-01-18T13:57:19.442135Z","shell.execute_reply":"2023-01-18T13:57:19.440980Z","shell.execute_reply.started":"2023-01-18T13:57:19.436373Z"},"id":"CxlxEAOgG-pn","outputId":"28dc6f19-7b0c-4d7f-e6c3-ff5271c7935a","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["13\n"]}],"source":["row = {\"tags\":\"horror, horror\"}\n","print(dict_map[row[\"tags\"].split(\", \")[0]])"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2023-01-18T13:57:19.579671Z","iopub.status.busy":"2023-01-18T13:57:19.579243Z","iopub.status.idle":"2023-01-18T13:57:19.586120Z","shell.execute_reply":"2023-01-18T13:57:19.584829Z","shell.execute_reply.started":"2023-01-18T13:57:19.579634Z"},"id":"zBzucov4GOnC","trusted":true},"outputs":[],"source":["def process_data(row):\n","    text = row[\"plot_synopsis\"]\n","    encodings = tokenizer(text,padding=\"max_length\",truncation=True,max_length=512)\n","    encodings[\"label\"] = dict_map[row[\"tags\"].split(\", \")[0]]\n","    encodings[\"text\"] = text\n","    return encodings "]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2023-01-18T13:57:19.726282Z","iopub.status.busy":"2023-01-18T13:57:19.725527Z","iopub.status.idle":"2023-01-18T13:57:19.738155Z","shell.execute_reply":"2023-01-18T13:57:19.737093Z","shell.execute_reply.started":"2023-01-18T13:57:19.726243Z"},"id":"enC2JFAxGOjV","outputId":"d1f39dcc-eafa-4f4d-eb4a-d6ef74d4ed3d","trusted":true},"outputs":[{"data":{"text/plain":["{'input_ids': [101, 2048, 4595, 2086, 3283, 1010, 18699, 8490, 6820, 5313, 1996, 12487, 1010, 1037, 23324, 2040, 7065, 12260, 2094, 1999, 13593, 2075, 1996, 7036, 1998, 1996, 3659, 1997, 13905, 1010, 20832, 1996, 2203, 1997, 2010, 9801, 2420, 1998, 2001, 20006, 2098, 1012, 10202, 2011, 11150, 2005, 1996, 2542, 1010, 18699, 8490, 6820, 5313, 2853, 2010, 3969, 2000, 1996, 5698, 8140, 1997, 1996, 22159, 2061, 2008, 2010, 16007, 16206, 4382, 2052, 5788, 1012, 1999, 2019, 4654, 26775, 14194, 15370, 8887, 1010, 18699, 8490, 6820, 12718, 3096, 2001, 13109, 4710, 2098, 2046, 5530, 1010, 2010, 5944, 25756, 2046, 1037, 3104, 1010, 1998, 2010, 4295, 2094, 2668, 2150, 1996, 10710, 2000, 7279, 1037, 2338, 2087, 25047, 1012, 7329, 25047, 1998, 2139, 18098, 10696, 2094, 3123, 2013, 2296, 6770, 1998, 4470, 2319, 15355, 2000, 2112, 13808, 1999, 1996, 9016, 1997, 6215, 1012, 1996, 12028, 1997, 10556, 8024, 14573, 2020, 10202, 2011, 2023, 11629, 1997, 4763, 2127, 2019, 2344, 1997, 4151, 6424, 10375, 2013, 1996, 11289, 1012, 1996, 7307, 1997, 1996, 2047, 3103, 12860, 2019, 11292, 2000, 24501, 3126, 2890, 6593, 3246, 1999, 1996, 2455, 1012, 1996, 18433, 1997, 2037, 8072, 2001, 2061, 2307, 2008, 21877, 10626, 1010, 1996, 2643, 1997, 2422, 1010, 2435, 1996, 7307, 3928, 25087, 2015, 2007, 2029, 2000, 3149, 2010, 2373, 1012, 9099, 23865, 4765, 2007, 7746, 2453, 1010, 1996, 7307, 1997, 1996, 2047, 3103, 16276, 1996, 5192, 2008, 2018, 13755, 1996, 2455, 2005, 4376, 3634, 2086, 1998, 3459, 2009, 2004, 20824, 1012, 2021, 2025, 2035, 2020, 15180, 2094, 2011, 2037, 8294, 1012, 1996, 17102, 1997, 18699, 8490, 6820, 5313, 4487, 20939, 3366, 23931, 1996, 2338, 1998, 26470, 2094, 2093, 20505, 9293, 2000, 5342, 1996, 4109, 2127, 2027, 2071, 2022, 5140, 1012, 1996, 10710, 2001, 3603, 1998, 3908, 2021, 1010, 2750, 2086, 1997, 6575, 1010, 1996, 3104, 1998, 5530, 2020, 2196, 2179, 1012, 3521, 5451, 1996, 2455, 2005, 4693, 1998, 1996, 7307, 2288, 2439, 1999, 1996, 2422, 1997, 2037, 2219, 8294, 1012, 2004, 3638, 1997, 1996, 9643, 2824, 8105, 2061, 2106, 1996, 2373, 1997, 8858, 1997, 21877, 10626, 1012, 2027, 4895, 9148, 13027, 2135, 4704, 3209, 1999, 1996, 16542, 6772, 2008, 1996, 2338, 1997, 25047, 4768, 2071, 2196, 2153, 2022, 2081, 2878, 1012, 2085, 1010, 1996, 3588, 4109, 2031, 2042, 3603, 1010, 1998, 2019, 3418, 4763, 2003, 7161, 2000, 3288, 2068, 2362, 1998, 9239, 1996, 24933, 1998, 1996, 4763, 2009, 2716, 1012, 2021, 2012, 1996, 2168, 2051, 1037, 4022, 2047, 14412, 17190, 2078, 2038, 2042, 2315, 2000, 1996, 7307, 1997, 1996, 2047, 3103, 2000, 3535, 2000, 20687, 2037, 2373, 2000, 2954, 2023, 4763, 1012, 2021, 1010, 2000, 2079, 2061, 1010, 2002, 2089, 2342, 2000, 2175, 2114, 2035, 2008, 2002, 2038, 2218, 6203, 1010, 26875, 2062, 2008, 2074, 2010, 2219, 3969, 1999, 2010, 8795, 2000, 6033, 1996, 4763, 2008, 20626, 2032, 2012, 2296, 2735, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'label': 31, 'text': 'Two thousand years ago, Nhagruul the Foul, a sorcerer who reveled in corrupting the innocent and the spread of despair, neared the end of his mortal days and was dismayed. Consumed by hatred for the living, Nhagruul sold his soul to the demon Lords of the abyss so that his malign spirit would survive. In an excruciating ritual, Nhagrulls skin was flayed into pages, his bones hammered into a cover, and his diseased blood became the ink to pen a book most vile. Creatures vile and depraved rose from every pit and unclean barrow to partake in the fever of destruction. The kingdoms of Karkoth were consumed by this plague of evil until an order of holy warriors arose from the ashes. The Knights of the New Sun swore an oath to resurrect hope in the land. The purity of their hearts was so great that Pelor, the God of Light, gave the Knights powerful amulets with which to channel his power. Transcendent with divine might, the Knights of the New Sun pierced the shadow that had darkened the land for twelve hundred years and cast it asunder. But not all were awed by their glory. The disciples of Nhagruul disassembled the book and bribed three greedy souls to hide the pieces until they could be retrieved. The ink was discovered and destroyed but, despite years of searching, the cover and pages were never found. Peace ruled the land for centuries and the Knights got lost in the light of their own glory. As memory of the awful events faded so did the power of servants of Pelor. They unwittingly abandoned themselves in the incorrect belief that the Book of Vile Darkness could never again be made whole.Now, the remaining pieces have been discovered, and an ancient evil is attempting to bring them together and restore the relic and the evil it brought. But at the same time a potential new paladin has been named to the Knights of the New Sun to attempt to renew their power to fight this evil. But, to do so, he may need to go against all that he has held dear, risking more that just his own soul in his quest to destroy the evil that surrounds him at every turn.'}"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["process_data({\n","'plot_synopsis':'Two thousand years ago, Nhagruul the Foul, a sorcerer who reveled in corrupting the innocent and the spread of despair, neared the end of his mortal days and was dismayed. Consumed by hatred for the living, Nhagruul sold his soul to the demon Lords of the abyss so that his malign spirit would survive. In an excruciating ritual, Nhagrulls skin was flayed into pages, his bones hammered into a cover, and his diseased blood became the ink to pen a book most vile. Creatures vile and depraved rose from every pit and unclean barrow to partake in the fever of destruction. The kingdoms of Karkoth were consumed by this plague of evil until an order of holy warriors arose from the ashes. The Knights of the New Sun swore an oath to resurrect hope in the land. The purity of their hearts was so great that Pelor, the God of Light, gave the Knights powerful amulets with which to channel his power. Transcendent with divine might, the Knights of the New Sun pierced the shadow that had darkened the land for twelve hundred years and cast it asunder. But not all were awed by their glory. The disciples of Nhagruul disassembled the book and bribed three greedy souls to hide the pieces until they could be retrieved. The ink was discovered and destroyed but, despite years of searching, the cover and pages were never found. Peace ruled the land for centuries and the Knights got lost in the light of their own glory. As memory of the awful events faded so did the power of servants of Pelor. They unwittingly abandoned themselves in the incorrect belief that the Book of Vile Darkness could never again be made whole.Now, the remaining pieces have been discovered, and an ancient evil is attempting to bring them together and restore the relic and the evil it brought. But at the same time a potential new paladin has been named to the Knights of the New Sun to attempt to renew their power to fight this evil. But, to do so, he may need to go against all that he has held dear, risking more that just his own soul in his quest to destroy the evil that surrounds him at every turn.',\n","'tags':'cult, horror, gothic, murder, atmospheric'\n","    \n","})"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2023-01-18T13:57:19.841124Z","iopub.status.busy":"2023-01-18T13:57:19.840707Z","iopub.status.idle":"2023-01-18T13:57:19.850839Z","shell.execute_reply":"2023-01-18T13:57:19.848989Z","shell.execute_reply.started":"2023-01-18T13:57:19.841090Z"},"id":"kzkyGuqSGOgy","trusted":true},"outputs":[],"source":["df.drop([\"imdb_id\",\"title\",\"split\",\"synopsis_source\"],axis=1,inplace=True)"]},{"cell_type":"code","execution_count":19,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2023-01-18T13:57:20.609440Z","iopub.status.busy":"2023-01-18T13:57:20.609062Z","iopub.status.idle":"2023-01-18T13:57:20.619822Z","shell.execute_reply":"2023-01-18T13:57:20.618813Z","shell.execute_reply.started":"2023-01-18T13:57:20.609407Z"},"id":"ZDDc6NTNGOeM","outputId":"41a1c8fc-4db0-4b4e-dedf-fac6fe4d99f8","trusted":true},"outputs":[{"data":{"text/plain":["Index(['plot_synopsis', 'tags'], dtype='object')"]},"execution_count":19,"metadata":{},"output_type":"execute_result"}],"source":["df.columns"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2023-01-18T13:57:21.261320Z","iopub.status.busy":"2023-01-18T13:57:21.260954Z","iopub.status.idle":"2023-01-18T13:58:12.451616Z","shell.execute_reply":"2023-01-18T13:58:12.450621Z","shell.execute_reply.started":"2023-01-18T13:57:21.261287Z"},"id":"v7Sjwex1GObu","trusted":true},"outputs":[],"source":["processed_data = []\n","for i in range(len(df)):\n","    processed_data.append(process_data(df.iloc[i]))\n"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2023-01-18T13:58:12.453849Z","iopub.status.busy":"2023-01-18T13:58:12.453479Z","iopub.status.idle":"2023-01-18T13:58:13.141323Z","shell.execute_reply":"2023-01-18T13:58:13.140359Z","shell.execute_reply.started":"2023-01-18T13:58:12.453812Z"},"id":"VpZRdPPoIzwX","trusted":true},"outputs":[],"source":["from datasets import Dataset"]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2023-01-18T13:58:13.143972Z","iopub.status.busy":"2023-01-18T13:58:13.143247Z","iopub.status.idle":"2023-01-18T13:58:13.513498Z","shell.execute_reply":"2023-01-18T13:58:13.512552Z","shell.execute_reply.started":"2023-01-18T13:58:13.143928Z"},"id":"nao_MT1VGOZZ","trusted":true},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","new_df = pd.DataFrame(processed_data)\n","train_df,valid_df = train_test_split(new_df,test_size=0.2,random_state=2021)"]},{"cell_type":"code","execution_count":23,"metadata":{"execution":{"iopub.execute_input":"2023-01-18T13:58:13.516231Z","iopub.status.busy":"2023-01-18T13:58:13.515875Z","iopub.status.idle":"2023-01-18T13:58:15.081376Z","shell.execute_reply":"2023-01-18T13:58:15.080430Z","shell.execute_reply.started":"2023-01-18T13:58:13.516195Z"},"id":"CWET5IbhGOXD","trusted":true},"outputs":[],"source":["import pyarrow as pa\n","from datasets import Dataset\n","\n","train_set = Dataset(pa.Table.from_pandas(train_df))\n","test_set = Dataset(pa.Table.from_pandas(valid_df))"]},{"cell_type":"code","execution_count":24,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2023-01-18T13:58:15.083333Z","iopub.status.busy":"2023-01-18T13:58:15.082978Z","iopub.status.idle":"2023-01-18T13:58:15.090080Z","shell.execute_reply":"2023-01-18T13:58:15.088934Z","shell.execute_reply.started":"2023-01-18T13:58:15.083297Z"},"id":"Pi2cGUhaGOTD","outputId":"47c2083d-4e6e-4944-cf25-dd6d52c9d5b7","trusted":true},"outputs":[{"data":{"text/plain":["datasets.arrow_dataset.Dataset"]},"execution_count":24,"metadata":{},"output_type":"execute_result"}],"source":["type(train_set)"]},{"cell_type":"code","execution_count":25,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2023-01-18T13:58:15.092773Z","iopub.status.busy":"2023-01-18T13:58:15.091965Z","iopub.status.idle":"2023-01-18T13:58:28.448469Z","shell.execute_reply":"2023-01-18T13:58:28.447505Z","shell.execute_reply.started":"2023-01-18T13:58:15.092736Z"},"id":"U8wYrKDKGOPj","outputId":"59a771ba-5e24-46c8-d54d-7a2cfd252147","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"09be93bfd48a454fbedeba77e16e40ef","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/420M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["from transformers import AutoModelForSequenceClassification\n","\n","model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\",num_labels=71)"]},{"cell_type":"code","execution_count":26,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2023-01-18T13:58:28.450906Z","iopub.status.busy":"2023-01-18T13:58:28.450470Z","iopub.status.idle":"2023-01-18T13:58:33.529242Z","shell.execute_reply":"2023-01-18T13:58:33.528154Z","shell.execute_reply.started":"2023-01-18T13:58:28.450863Z"},"id":"bMGDdfJqIb9i","outputId":"9db5913e-dc9a-4957-d880-cdb391308883","trusted":true},"outputs":[],"source":["from transformers import TrainingArguments, Trainer\n","\n","training_args = TrainingArguments(output_dir=\"./results\",evaluation_strategy=\"epoch\",num_train_epochs=5)\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=train_set,\n","    eval_dataset=test_set,\n","    tokenizer=tokenizer,\n",")"]},{"cell_type":"code","execution_count":27,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":612},"execution":{"iopub.execute_input":"2023-01-18T13:58:33.532423Z","iopub.status.busy":"2023-01-18T13:58:33.530808Z","iopub.status.idle":"2023-01-18T15:01:01.202344Z","shell.execute_reply":"2023-01-18T15:01:01.201363Z","shell.execute_reply.started":"2023-01-18T13:58:33.532380Z"},"id":"2f_AXwA6Ib7i","outputId":"0e34313d-ce5f-4134-848a-fd3e45dc73a3","trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, text. If __index_level_0__, text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n","/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 11862\n","  Num Epochs = 5\n","  Instantaneous batch size per device = 8\n","  Total train batch size (w. parallel, distributed & accumulation) = 8\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 7415\n","Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n","\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n","\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"]},{"name":"stdout","output_type":"stream","text":["  ········································\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"]},{"name":"stdout","output_type":"stream","text":["huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"]},{"data":{"text/html":["wandb version 0.13.9 is available!  To upgrade, please run:\n"," $ pip install wandb --upgrade"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Tracking run with wandb version 0.12.21"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/kaggle/working/wandb/run-20230118_140035-22wdbqiv</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href=\"https://wandb.ai/empyrean/huggingface/runs/22wdbqiv\" target=\"_blank\">./results</a></strong> to <a href=\"https://wandb.ai/empyrean/huggingface\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='7415' max='7415' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [7415/7415 1:00:16, Epoch 5/5]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>3.151100</td>\n","      <td>2.985774</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>2.855000</td>\n","      <td>2.902964</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>2.473900</td>\n","      <td>3.038890</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>1.976200</td>\n","      <td>3.208399</td>\n","    </tr>\n","    <tr>\n","      <td>5</td>\n","      <td>1.556300</td>\n","      <td>3.413345</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Saving model checkpoint to ./results/checkpoint-500\n","Configuration saved in ./results/checkpoint-500/config.json\n","Model weights saved in ./results/checkpoint-500/pytorch_model.bin\n","tokenizer config file saved in ./results/checkpoint-500/tokenizer_config.json\n","Special tokens file saved in ./results/checkpoint-500/special_tokens_map.json\n","Saving model checkpoint to ./results/checkpoint-1000\n","Configuration saved in ./results/checkpoint-1000/config.json\n","Model weights saved in ./results/checkpoint-1000/pytorch_model.bin\n","tokenizer config file saved in ./results/checkpoint-1000/tokenizer_config.json\n","Special tokens file saved in ./results/checkpoint-1000/special_tokens_map.json\n","The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, text. If __index_level_0__, text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 2966\n","  Batch size = 8\n","Saving model checkpoint to ./results/checkpoint-1500\n","Configuration saved in ./results/checkpoint-1500/config.json\n","Model weights saved in ./results/checkpoint-1500/pytorch_model.bin\n","tokenizer config file saved in ./results/checkpoint-1500/tokenizer_config.json\n","Special tokens file saved in ./results/checkpoint-1500/special_tokens_map.json\n","Saving model checkpoint to ./results/checkpoint-2000\n","Configuration saved in ./results/checkpoint-2000/config.json\n","Model weights saved in ./results/checkpoint-2000/pytorch_model.bin\n","tokenizer config file saved in ./results/checkpoint-2000/tokenizer_config.json\n","Special tokens file saved in ./results/checkpoint-2000/special_tokens_map.json\n","Saving model checkpoint to ./results/checkpoint-2500\n","Configuration saved in ./results/checkpoint-2500/config.json\n","Model weights saved in ./results/checkpoint-2500/pytorch_model.bin\n","tokenizer config file saved in ./results/checkpoint-2500/tokenizer_config.json\n","Special tokens file saved in ./results/checkpoint-2500/special_tokens_map.json\n","The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, text. If __index_level_0__, text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 2966\n","  Batch size = 8\n","Saving model checkpoint to ./results/checkpoint-3000\n","Configuration saved in ./results/checkpoint-3000/config.json\n","Model weights saved in ./results/checkpoint-3000/pytorch_model.bin\n","tokenizer config file saved in ./results/checkpoint-3000/tokenizer_config.json\n","Special tokens file saved in ./results/checkpoint-3000/special_tokens_map.json\n","Saving model checkpoint to ./results/checkpoint-3500\n","Configuration saved in ./results/checkpoint-3500/config.json\n","Model weights saved in ./results/checkpoint-3500/pytorch_model.bin\n","tokenizer config file saved in ./results/checkpoint-3500/tokenizer_config.json\n","Special tokens file saved in ./results/checkpoint-3500/special_tokens_map.json\n","Saving model checkpoint to ./results/checkpoint-4000\n","Configuration saved in ./results/checkpoint-4000/config.json\n","Model weights saved in ./results/checkpoint-4000/pytorch_model.bin\n","tokenizer config file saved in ./results/checkpoint-4000/tokenizer_config.json\n","Special tokens file saved in ./results/checkpoint-4000/special_tokens_map.json\n","The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, text. If __index_level_0__, text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 2966\n","  Batch size = 8\n","Saving model checkpoint to ./results/checkpoint-4500\n","Configuration saved in ./results/checkpoint-4500/config.json\n","Model weights saved in ./results/checkpoint-4500/pytorch_model.bin\n","tokenizer config file saved in ./results/checkpoint-4500/tokenizer_config.json\n","Special tokens file saved in ./results/checkpoint-4500/special_tokens_map.json\n","Saving model checkpoint to ./results/checkpoint-5000\n","Configuration saved in ./results/checkpoint-5000/config.json\n","Model weights saved in ./results/checkpoint-5000/pytorch_model.bin\n","tokenizer config file saved in ./results/checkpoint-5000/tokenizer_config.json\n","Special tokens file saved in ./results/checkpoint-5000/special_tokens_map.json\n","Saving model checkpoint to ./results/checkpoint-5500\n","Configuration saved in ./results/checkpoint-5500/config.json\n","Model weights saved in ./results/checkpoint-5500/pytorch_model.bin\n","tokenizer config file saved in ./results/checkpoint-5500/tokenizer_config.json\n","Special tokens file saved in ./results/checkpoint-5500/special_tokens_map.json\n","The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, text. If __index_level_0__, text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 2966\n","  Batch size = 8\n","Saving model checkpoint to ./results/checkpoint-6000\n","Configuration saved in ./results/checkpoint-6000/config.json\n","Model weights saved in ./results/checkpoint-6000/pytorch_model.bin\n","tokenizer config file saved in ./results/checkpoint-6000/tokenizer_config.json\n","Special tokens file saved in ./results/checkpoint-6000/special_tokens_map.json\n","Saving model checkpoint to ./results/checkpoint-6500\n","Configuration saved in ./results/checkpoint-6500/config.json\n","Model weights saved in ./results/checkpoint-6500/pytorch_model.bin\n","tokenizer config file saved in ./results/checkpoint-6500/tokenizer_config.json\n","Special tokens file saved in ./results/checkpoint-6500/special_tokens_map.json\n","Saving model checkpoint to ./results/checkpoint-7000\n","Configuration saved in ./results/checkpoint-7000/config.json\n","Model weights saved in ./results/checkpoint-7000/pytorch_model.bin\n","tokenizer config file saved in ./results/checkpoint-7000/tokenizer_config.json\n","Special tokens file saved in ./results/checkpoint-7000/special_tokens_map.json\n","The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, text. If __index_level_0__, text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 2966\n","  Batch size = 8\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n"]},{"data":{"text/plain":["TrainOutput(global_step=7415, training_loss=2.423611378171359, metrics={'train_runtime': 3747.6325, 'train_samples_per_second': 15.826, 'train_steps_per_second': 1.979, 'total_flos': 1.561478442458112e+16, 'train_loss': 2.423611378171359, 'epoch': 5.0})"]},"execution_count":27,"metadata":{},"output_type":"execute_result"}],"source":["####ADD BATCHING BEFORE TRAINING TO REDUCE PARAMS\n","trainer.train()"]},{"cell_type":"code","execution_count":28,"metadata":{"execution":{"iopub.execute_input":"2023-01-18T15:01:01.204744Z","iopub.status.busy":"2023-01-18T15:01:01.204101Z","iopub.status.idle":"2023-01-18T15:01:51.990077Z","shell.execute_reply":"2023-01-18T15:01:51.989178Z","shell.execute_reply.started":"2023-01-18T15:01:01.204704Z"},"id":"B05M5rB4Ib5F","trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, text. If __index_level_0__, text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 2966\n","  Batch size = 8\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='371' max='371' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [371/371 00:50]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["{'eval_loss': 3.413344621658325,\n"," 'eval_runtime': 50.7644,\n"," 'eval_samples_per_second': 58.427,\n"," 'eval_steps_per_second': 7.308,\n"," 'epoch': 5.0}"]},"execution_count":28,"metadata":{},"output_type":"execute_result"}],"source":["trainer.evaluate()"]},{"cell_type":"code","execution_count":29,"metadata":{"execution":{"iopub.execute_input":"2023-01-18T15:01:51.998494Z","iopub.status.busy":"2023-01-18T15:01:51.995925Z","iopub.status.idle":"2023-01-18T15:01:53.026459Z","shell.execute_reply":"2023-01-18T15:01:53.025501Z","shell.execute_reply.started":"2023-01-18T15:01:51.998456Z"},"id":"eBptUxxaIb2H","trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Configuration saved in ./model/config.json\n","Model weights saved in ./model/pytorch_model.bin\n"]}],"source":["model.save_pretrained(\"./model/\")"]},{"cell_type":"code","execution_count":30,"metadata":{"execution":{"iopub.execute_input":"2023-01-18T15:01:53.028411Z","iopub.status.busy":"2023-01-18T15:01:53.028049Z","iopub.status.idle":"2023-01-18T15:01:54.626108Z","shell.execute_reply":"2023-01-18T15:01:54.625137Z","shell.execute_reply.started":"2023-01-18T15:01:53.028376Z"},"id":"kgnVl-piIbzW","trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["loading configuration file ./model/config.json\n","Model config BertConfig {\n","  \"_name_or_path\": \"./model/\",\n","  \"architectures\": [\n","    \"BertForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\",\n","    \"3\": \"LABEL_3\",\n","    \"4\": \"LABEL_4\",\n","    \"5\": \"LABEL_5\",\n","    \"6\": \"LABEL_6\",\n","    \"7\": \"LABEL_7\",\n","    \"8\": \"LABEL_8\",\n","    \"9\": \"LABEL_9\",\n","    \"10\": \"LABEL_10\",\n","    \"11\": \"LABEL_11\",\n","    \"12\": \"LABEL_12\",\n","    \"13\": \"LABEL_13\",\n","    \"14\": \"LABEL_14\",\n","    \"15\": \"LABEL_15\",\n","    \"16\": \"LABEL_16\",\n","    \"17\": \"LABEL_17\",\n","    \"18\": \"LABEL_18\",\n","    \"19\": \"LABEL_19\",\n","    \"20\": \"LABEL_20\",\n","    \"21\": \"LABEL_21\",\n","    \"22\": \"LABEL_22\",\n","    \"23\": \"LABEL_23\",\n","    \"24\": \"LABEL_24\",\n","    \"25\": \"LABEL_25\",\n","    \"26\": \"LABEL_26\",\n","    \"27\": \"LABEL_27\",\n","    \"28\": \"LABEL_28\",\n","    \"29\": \"LABEL_29\",\n","    \"30\": \"LABEL_30\",\n","    \"31\": \"LABEL_31\",\n","    \"32\": \"LABEL_32\",\n","    \"33\": \"LABEL_33\",\n","    \"34\": \"LABEL_34\",\n","    \"35\": \"LABEL_35\",\n","    \"36\": \"LABEL_36\",\n","    \"37\": \"LABEL_37\",\n","    \"38\": \"LABEL_38\",\n","    \"39\": \"LABEL_39\",\n","    \"40\": \"LABEL_40\",\n","    \"41\": \"LABEL_41\",\n","    \"42\": \"LABEL_42\",\n","    \"43\": \"LABEL_43\",\n","    \"44\": \"LABEL_44\",\n","    \"45\": \"LABEL_45\",\n","    \"46\": \"LABEL_46\",\n","    \"47\": \"LABEL_47\",\n","    \"48\": \"LABEL_48\",\n","    \"49\": \"LABEL_49\",\n","    \"50\": \"LABEL_50\",\n","    \"51\": \"LABEL_51\",\n","    \"52\": \"LABEL_52\",\n","    \"53\": \"LABEL_53\",\n","    \"54\": \"LABEL_54\",\n","    \"55\": \"LABEL_55\",\n","    \"56\": \"LABEL_56\",\n","    \"57\": \"LABEL_57\",\n","    \"58\": \"LABEL_58\",\n","    \"59\": \"LABEL_59\",\n","    \"60\": \"LABEL_60\",\n","    \"61\": \"LABEL_61\",\n","    \"62\": \"LABEL_62\",\n","    \"63\": \"LABEL_63\",\n","    \"64\": \"LABEL_64\",\n","    \"65\": \"LABEL_65\",\n","    \"66\": \"LABEL_66\",\n","    \"67\": \"LABEL_67\",\n","    \"68\": \"LABEL_68\",\n","    \"69\": \"LABEL_69\",\n","    \"70\": \"LABEL_70\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_10\": 10,\n","    \"LABEL_11\": 11,\n","    \"LABEL_12\": 12,\n","    \"LABEL_13\": 13,\n","    \"LABEL_14\": 14,\n","    \"LABEL_15\": 15,\n","    \"LABEL_16\": 16,\n","    \"LABEL_17\": 17,\n","    \"LABEL_18\": 18,\n","    \"LABEL_19\": 19,\n","    \"LABEL_2\": 2,\n","    \"LABEL_20\": 20,\n","    \"LABEL_21\": 21,\n","    \"LABEL_22\": 22,\n","    \"LABEL_23\": 23,\n","    \"LABEL_24\": 24,\n","    \"LABEL_25\": 25,\n","    \"LABEL_26\": 26,\n","    \"LABEL_27\": 27,\n","    \"LABEL_28\": 28,\n","    \"LABEL_29\": 29,\n","    \"LABEL_3\": 3,\n","    \"LABEL_30\": 30,\n","    \"LABEL_31\": 31,\n","    \"LABEL_32\": 32,\n","    \"LABEL_33\": 33,\n","    \"LABEL_34\": 34,\n","    \"LABEL_35\": 35,\n","    \"LABEL_36\": 36,\n","    \"LABEL_37\": 37,\n","    \"LABEL_38\": 38,\n","    \"LABEL_39\": 39,\n","    \"LABEL_4\": 4,\n","    \"LABEL_40\": 40,\n","    \"LABEL_41\": 41,\n","    \"LABEL_42\": 42,\n","    \"LABEL_43\": 43,\n","    \"LABEL_44\": 44,\n","    \"LABEL_45\": 45,\n","    \"LABEL_46\": 46,\n","    \"LABEL_47\": 47,\n","    \"LABEL_48\": 48,\n","    \"LABEL_49\": 49,\n","    \"LABEL_5\": 5,\n","    \"LABEL_50\": 50,\n","    \"LABEL_51\": 51,\n","    \"LABEL_52\": 52,\n","    \"LABEL_53\": 53,\n","    \"LABEL_54\": 54,\n","    \"LABEL_55\": 55,\n","    \"LABEL_56\": 56,\n","    \"LABEL_57\": 57,\n","    \"LABEL_58\": 58,\n","    \"LABEL_59\": 59,\n","    \"LABEL_6\": 6,\n","    \"LABEL_60\": 60,\n","    \"LABEL_61\": 61,\n","    \"LABEL_62\": 62,\n","    \"LABEL_63\": 63,\n","    \"LABEL_64\": 64,\n","    \"LABEL_65\": 65,\n","    \"LABEL_66\": 66,\n","    \"LABEL_67\": 67,\n","    \"LABEL_68\": 68,\n","    \"LABEL_69\": 69,\n","    \"LABEL_7\": 7,\n","    \"LABEL_70\": 70,\n","    \"LABEL_8\": 8,\n","    \"LABEL_9\": 9\n","  },\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"problem_type\": \"single_label_classification\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.20.1\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file ./model/pytorch_model.bin\n","All model checkpoint weights were used when initializing BertForSequenceClassification.\n","\n","All the weights of BertForSequenceClassification were initialized from the model checkpoint at ./model/.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n"]}],"source":["from transformers import AutoModelForSequenceClassification\n","new_model = AutoModelForSequenceClassification.from_pretrained(\"./model/\")"]},{"cell_type":"code","execution_count":31,"metadata":{"execution":{"iopub.execute_input":"2023-01-18T15:01:54.628444Z","iopub.status.busy":"2023-01-18T15:01:54.627399Z","iopub.status.idle":"2023-01-18T15:01:58.850943Z","shell.execute_reply":"2023-01-18T15:01:58.849990Z","shell.execute_reply.started":"2023-01-18T15:01:54.628396Z"},"id":"98wscWiHDaIw","trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n","Model config BertConfig {\n","  \"_name_or_path\": \"bert-base-uncased\",\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.20.1\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n","loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n","Model config BertConfig {\n","  \"_name_or_path\": \"bert-base-uncased\",\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.20.1\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n"]}],"source":["from transformers import AutoTokenizer\n","tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")"]},{"cell_type":"code","execution_count":32,"metadata":{"execution":{"iopub.execute_input":"2023-01-18T15:01:58.852635Z","iopub.status.busy":"2023-01-18T15:01:58.852263Z","iopub.status.idle":"2023-01-18T15:01:58.861748Z","shell.execute_reply":"2023-01-18T15:01:58.860618Z","shell.execute_reply.started":"2023-01-18T15:01:58.852597Z"},"id":"gpvJ3VYTDmOZ","trusted":true},"outputs":[],"source":["import torch\n","import numpy as np\n","\n","def get_prediction(text):\n","  encoding =tokenizer(text,return_tensors=\"pt\",padding=\"max_length\",truncation=True,max_length=512)\n","  encoding = {k: v.to(\"cpu\") for k,v in encoding.items()}\n","\n","  outputs = new_model(**encoding)\n","  logits = outputs.logits\n","  softmax = torch.nn.Softmax()\n","  probs = softmax(logits.squeeze().cpu())\n","  probs = probs.detach().numpy()\n","  labels = np.argmax(probs,axis=-1)\n","  return {\"tags\":labels}\n"]},{"cell_type":"code","execution_count":33,"metadata":{"execution":{"iopub.execute_input":"2023-01-18T15:01:58.864359Z","iopub.status.busy":"2023-01-18T15:01:58.863599Z","iopub.status.idle":"2023-01-18T15:01:58.888909Z","shell.execute_reply":"2023-01-18T15:01:58.887691Z","shell.execute_reply.started":"2023-01-18T15:01:58.864322Z"},"id":"kj8zai8nFmC0","trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>imdb_id</th>\n","      <th>title</th>\n","      <th>plot_synopsis</th>\n","      <th>tags</th>\n","      <th>split</th>\n","      <th>synopsis_source</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>2</th>\n","      <td>tt0033045</td>\n","      <td>The Shop Around the Corner</td>\n","      <td>Matuschek's, a gift store in Budapest, is the ...</td>\n","      <td>romantic</td>\n","      <td>test</td>\n","      <td>imdb</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>tt1937113</td>\n","      <td>Call of Duty: Modern Warfare 3</td>\n","      <td>Hours after the end of the previous game and t...</td>\n","      <td>good versus evil</td>\n","      <td>test</td>\n","      <td>imdb</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>tt0102007</td>\n","      <td>The Haunted</td>\n","      <td>This creepy and scary story centers around The...</td>\n","      <td>paranormal, horror, haunting</td>\n","      <td>test</td>\n","      <td>imdb</td>\n","    </tr>\n","    <tr>\n","      <th>24</th>\n","      <td>tt2005374</td>\n","      <td>The Frozen Ground</td>\n","      <td>The film opens in an Anchorage motel room in 1...</td>\n","      <td>dramatic, murder</td>\n","      <td>test</td>\n","      <td>wikipedia</td>\n","    </tr>\n","    <tr>\n","      <th>27</th>\n","      <td>tt1411238</td>\n","      <td>No Strings Attached</td>\n","      <td>15 years agoWe see two young kids, named Emma ...</td>\n","      <td>boring, adult comedy, cute, flashback, romanti...</td>\n","      <td>test</td>\n","      <td>imdb</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["      imdb_id                           title  \\\n","2   tt0033045      The Shop Around the Corner   \n","15  tt1937113  Call of Duty: Modern Warfare 3   \n","19  tt0102007                     The Haunted   \n","24  tt2005374               The Frozen Ground   \n","27  tt1411238             No Strings Attached   \n","\n","                                        plot_synopsis  \\\n","2   Matuschek's, a gift store in Budapest, is the ...   \n","15  Hours after the end of the previous game and t...   \n","19  This creepy and scary story centers around The...   \n","24  The film opens in an Anchorage motel room in 1...   \n","27  15 years agoWe see two young kids, named Emma ...   \n","\n","                                                 tags split synopsis_source  \n","2                                            romantic  test            imdb  \n","15                                   good versus evil  test            imdb  \n","19                       paranormal, horror, haunting  test            imdb  \n","24                                   dramatic, murder  test       wikipedia  \n","27  boring, adult comedy, cute, flashback, romanti...  test            imdb  "]},"execution_count":33,"metadata":{},"output_type":"execute_result"}],"source":["test.head()"]},{"cell_type":"code","execution_count":53,"metadata":{"execution":{"iopub.execute_input":"2023-01-18T15:05:10.381538Z","iopub.status.busy":"2023-01-18T15:05:10.381163Z","iopub.status.idle":"2023-01-18T15:05:10.394158Z","shell.execute_reply":"2023-01-18T15:05:10.392771Z","shell.execute_reply.started":"2023-01-18T15:05:10.381506Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["The film opens in an Anchorage motel room in 1983, where 17-year-old Cindy Paulson (Vanessa Hudgens) is handcuffed and screaming for help. She is rescued by an Anchorage Police Department patrol officer. He takes Paulson to the hospital, and her clothes are kept for a rape kit. At an APD station, she explains to detectives that she was abducted and raped. Because she is a prostitute and lying about her age, the detectives do not believe her story, refusing to even look into the man whom she named as her abductor, Robert Hansen (John Cusack). They claim Hansen is an upstanding member of society, a family man who owns his own restaurant, and has alibis from three people.\n","The APD patrol officer who rescued Paulson is outraged that the detectives refuse to pursue Hansen. He surreptitiously photocopies information about the case and sends it to the Alaska State Troopers. Meanwhile, state trooper Jack Halcombe (Nicolas Cage) has been called to investigate a female body that was found in the bush, half eaten by bears. The police connect the case to other missing girls, who have disappeared after going to what they thought were legitimate photo shoots. With secret information from the APD officer, Halcombe connects the other cases to Paulson's and starts to put together a portrait of Hansen. Paulson details how Hansen kept her captive, and that she escaped from his car when he tried to transfer her to his bush plane.\n","Meanwhile in Anchorage, Debbie Peters gets picked up by a man in an RV for a photo shoot. Later, Hansen eats a quiet dinner at home. His wife and children are away, and Hansen relaxes in his trophy room, casually ignoring Debbie who is chained to a post. She has urinated on the floor, and as she cleans up the mess with a towel, Hansen's neighbor enters the house to deliver a plate of food. Hansen warns Debbie not to scream and leaves the trophy room to greet his neighbor. Hansen then takes Debbie to the airport, where he orders her into his plane. After landing in a remote spot in the bush, Hansen frees Debbie, letting her run in a panic through the woods before he shoots her with a .223 caliber rifle. He steals her necklace before finishing her off with a handgun.\n","Halcombe has a very difficult time assembling a case against Hansen. Because the evidence is circumstantial and Paulson is afraid to testify, the district attorney refuses to issue a search warrant. Paulson keeps falling back into the world of stripping and prostitution, despite Halcombe's efforts to keep her safe. At a strip club, while she is trying to sell lap dances, she notices Hansen trolling for a new victim. Their eyes meet, a chase ensues, and Paulson barely escapes. The encounter makes Hansen nervous, and he hires Carl Galenski to find and kill Paulson. Carl approaches Paulson's erstwhile pimp Clate Johnson (50 Cent) and offers to forgive his sizable debt if Clate turns Paulson over to him.\n","Halcombe stakes out Hansen's house, causing Hansen to panic. Hansen gathers the evidence of his crimes, including the keepsakes from his victims, and flees with his son to the airport. He flies his plane to the bush and hides his keepsakes. Feeling that the chance to catch Hansen is slipping away, and with the victim count now at 17 girls, Halcombe forces the DA to issue a warrant. The search of Hansen's house yields no evidence, not even in his trophy room. Hansen agrees to be interrogated without a lawyer, but he is not yielding any new evidence. Halcombe arrests Hansen, but unless the police find new evidence, they will be unable to hold him.\n","Halcombe orders a second search of Hansen's house, which turns up a hidden cache of guns, including the .223 caliber rifle used in many of the murders. Under police watch at a safe location, Paulson slips away and returns to her life of prostitution. Clate picks her up and delivers her to Carl. When Clate attempts to rob Carl, Paulson uses the opportunity to escape, with Carl in pursuit. After making a call to Halcombe, Paulson is almost killed by Carl, but Halcombe rescues her just in time.\n","Halcombe uses a bracelet identical to one worn by one of the victims in order to trick Hansen into thinking the police have found where he hid the evidence in the bush. The bracelet, combined with the sight of Paulson in the interrogation room, enrages Hansen to the point that he incriminates himself. The epilogue states that Hansen confessed to the murder of 17 women and the kidnap and rape of another 30. He was charged with the abduction and rape of Cindy Paulson and the murders of Joanna Messina, Sherry Morrow, Paula Goulding and \"Eklutna Annie\". He was sentenced to 461 years plus life without parole. Robert Hansen died in 2014 at the age of 75. Only 11 of his victim's bodies were recovered. Jack Halcombe remained with the Alaska State Troopers and was promoted. He is now retired. Cindy Paulson now lives in the lower 48 and married with 3 children.\n","The film ends with a dedication and actual pictures of Hansen's victims.\n"]}],"source":["print(test[\"plot_synopsis\"][24])"]},{"cell_type":"code","execution_count":55,"metadata":{"execution":{"iopub.execute_input":"2023-01-18T15:05:19.347672Z","iopub.status.busy":"2023-01-18T15:05:19.346770Z","iopub.status.idle":"2023-01-18T15:05:20.465481Z","shell.execute_reply":"2023-01-18T15:05:20.464503Z","shell.execute_reply.started":"2023-01-18T15:05:19.347636Z"},"id":"0YfYg923Epxz","trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:11: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  # This is added back by InteractiveShellApp.init_path()\n"]},{"data":{"text/plain":["{'tags': 33}"]},"execution_count":55,"metadata":{},"output_type":"execute_result"}],"source":["get_prediction(test[\"plot_synopsis\"][24])"]},{"cell_type":"code","execution_count":56,"metadata":{"execution":{"iopub.execute_input":"2023-01-18T15:05:20.467169Z","iopub.status.busy":"2023-01-18T15:05:20.466897Z","iopub.status.idle":"2023-01-18T15:05:20.479637Z","shell.execute_reply":"2023-01-18T15:05:20.478610Z","shell.execute_reply.started":"2023-01-18T15:05:20.467143Z"},"id":"IoTVgU-7FGXc","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["{'historical fiction': 0, 'allegory': 1, 'suspenseful': 2, 'alternate history': 3, 'inspiring': 4, 'humor': 5, 'neo noir': 6, 'mystery': 7, 'boring': 8, 'sadist': 9, 'feel-good': 10, 'bleak': 11, 'haunting': 12, 'horror': 13, 'good versus evil': 14, 'cute': 15, 'avant garde': 16, 'dark': 17, 'clever': 18, 'sci-fi': 19, 'atmospheric': 20, 'melodrama': 21, 'murder': 22, 'christian film': 23, 'plot twist': 24, 'brainwashing': 25, 'philosophical': 26, 'paranormal': 27, 'anti war': 28, 'absurd': 29, 'psychedelic': 30, 'cult': 31, 'tragedy': 32, 'dramatic': 33, 'thought-provoking': 34, 'non fiction': 35, 'queer': 36, 'adult comedy': 37, 'comedy': 38, 'entertaining': 39, 'prank': 40, 'whimsical': 41, 'intrigue': 42, 'revenge': 43, 'historical': 44, 'claustrophobic': 45, 'autobiographical': 46, 'insanity': 47, 'realism': 48, 'action': 49, 'pornographic': 50, 'flashback': 51, 'home movie': 52, 'depressing': 53, 'cruelty': 54, 'satire': 55, 'stupid': 56, 'suicidal': 57, 'psychological': 58, 'romantic': 59, 'alternate reality': 60, 'violence': 61, 'storytelling': 62, 'grindhouse film': 63, 'western': 64, 'magical realism': 65, 'blaxploitation': 66, 'fantasy': 67, 'gothic': 68, 'sentimental': 69, 'comic': 70}\n"]}],"source":["print(dict_map)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Uz79Fh4AGq5z"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.13"},"vscode":{"interpreter":{"hash":"ea9ceb0b7e3f457d29129ad6ad6d3750fd1ab50a0da3c37c2bb5b7c85d56784f"}}},"nbformat":4,"nbformat_minor":4}
